{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zg1M5FZLlfdR",
        "outputId": "bc622519-aa66-413a-c47e-11f49396580b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (3.0.11)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting git+https://github.com/facebookresearch/fvcore.git\n",
            "  Cloning https://github.com/facebookresearch/fvcore.git to /tmp/pip-req-build-uhsgakdi\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/fvcore.git /tmp/pip-req-build-uhsgakdi\n",
            "  Resolved https://github.com/facebookresearch/fvcore.git to commit a491d5b9a06746f387aca2f1f9c7c7f28e20bef9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-wa2b0c06\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-wa2b0c06\n",
            "  Resolved https://github.com/cocodataset/cocoapi.git to commit 8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore==0.1.6) (1.26.4)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fvcore==0.1.6) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore==0.1.6) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore==0.1.6) (4.66.6)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore==0.1.6) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore==0.1.6) (11.0.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore==0.1.6) (0.9.0)\n",
            "Requirement already satisfied: iopath>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from fvcore==0.1.6) (0.1.9)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0) (75.1.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0) (3.0.11)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0) (3.8.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore==0.1.6) (3.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools==2.0) (1.16.0)\n",
            "Building wheels for collected packages: fvcore, pycocotools\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.6-py3-none-any.whl size=65670 sha256=59c926bba38e77dbd24acf2b3f47bc891093c0797f9eaf8105af076729c04055\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yffbnosw/wheels/8f/cb/6a/3b7ac0e01781855ca3d1417ebf9e15e20d5b7fe37ab063aa50\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp310-cp310-linux_x86_64.whl size=376060 sha256=9971a21410f27fb112c3ae26728349385fe93c49e6b51fe1896bff57b46bde79\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yffbnosw/wheels/39/61/b4/480fbddb4d3d6bc34083e7397bc6f5d1381f79acc68e9f3511\n",
            "Successfully built fvcore pycocotools\n",
            "Installing collected packages: fvcore, pycocotools\n",
            "  Attempting uninstall: fvcore\n",
            "    Found existing installation: fvcore 0.1.5.post20221221\n",
            "    Uninstalling fvcore-0.1.5.post20221221:\n",
            "      Successfully uninstalled fvcore-0.1.5.post20221221\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.8\n",
            "    Uninstalling pycocotools-2.0.8:\n",
            "      Successfully uninstalled pycocotools-2.0.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "detectron2 0.6 requires fvcore<0.1.6,>=0.1.5, but you have fvcore 0.1.6 which is incompatible.\n",
            "detectron2 0.6 requires pycocotools>=2.0.2, but you have pycocotools 2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fvcore-0.1.6 pycocotools-2.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.5.1+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# install dependencies\n",
        "!pip install -U torch torchvision cython\n",
        "!pip install -U 'git+https://github.com/facebookresearch/fvcore.git' 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "import torch, torchvision\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n",
        "!pip install -e detectron2_repo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UXugi5Alkli",
        "outputId": "cdbf6420-f5ed-44cf-cc72-e5f361f07735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'detectron2_repo' already exists and is not an empty directory.\n",
            "Obtaining file:///content/detectron2_repo\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (11.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.8.0)\n",
            "Collecting pycocotools>=2.0.2 (from detectron2==0.6)\n",
            "  Using cached pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.5.0)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.1.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.6)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.17.1)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Using cached fvcore-0.1.5.post20221221-py3-none-any.whl\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.9)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (24.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (3.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (1.0.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.3.6)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.12.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.68.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.2)\n",
            "Using cached pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
            "Installing collected packages: fvcore, pycocotools, detectron2\n",
            "  Attempting uninstall: fvcore\n",
            "    Found existing installation: fvcore 0.1.6\n",
            "    Uninstalling fvcore-0.1.6:\n",
            "      Successfully uninstalled fvcore-0.1.6\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0\n",
            "    Uninstalling pycocotools-2.0:\n",
            "      Successfully uninstalled pycocotools-2.0\n",
            "  Attempting uninstall: detectron2\n",
            "    Found existing installation: detectron2 0.6\n",
            "    Uninstalling detectron2-0.6:\n",
            "      Successfully uninstalled detectron2-0.6\n",
            "  Running setup.py develop for detectron2\n",
            "Successfully installed detectron2-0.6 fvcore-0.1.5.post20221221 pycocotools-2.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You may need to restart your runtime prior to this, to let your installation take effect\n",
        "# Some basic setup\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog"
      ],
      "metadata": {
        "id": "V7gMpkggloww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Google Drive 마운트\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9ltu-00lyel",
        "outputId": "abbc5cae-a30d-4a91-93d8-99efe7a042f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "def merge_json_files(json_dir, image_dir, output_json):\n",
        "    \"\"\"\n",
        "    여러 개의 JSON 파일을 COCO 형식으로 병합.\n",
        "    Args:\n",
        "        json_dir: JSON 파일들이 저장된 디렉토리 경로\n",
        "        image_dir: 이미지 파일들이 저장된 디렉토리 경로\n",
        "        output_json: 병합된 JSON 파일을 저장할 경로\n",
        "    \"\"\"\n",
        "    coco_format = {\n",
        "        \"images\": [],\n",
        "        \"annotations\": [],\n",
        "        \"categories\": []\n",
        "    }\n",
        "    annotation_id = 1\n",
        "    category_mapping = {}\n",
        "\n",
        "    # JSON 파일 처리\n",
        "    for json_file in os.listdir(json_dir):\n",
        "        if not json_file.endswith('.json'):\n",
        "            continue\n",
        "\n",
        "        with open(os.path.join(json_dir, json_file), 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # 이미지 정보 추가\n",
        "        image_info = data[\"images\"][0]  # 단일 이미지\n",
        "        coco_format[\"images\"].append(image_info)\n",
        "\n",
        "        # 주석 정보 추가\n",
        "        for annotation in data[\"annotations\"]:\n",
        "            annotation[\"id\"] = annotation_id\n",
        "            annotation_id += 1\n",
        "            coco_format[\"annotations\"].append(annotation)\n",
        "\n",
        "        # 카테고리 추가\n",
        "        for category in data[\"categories\"]:\n",
        "            if category[\"id\"] not in category_mapping:\n",
        "                coco_format[\"categories\"].append(category)\n",
        "                category_mapping[category[\"id\"]] = category[\"name\"]\n",
        "\n",
        "    # COCO 형식 JSON 저장\n",
        "    with open(output_json, 'w') as f:\n",
        "        json.dump(coco_format, f, indent=4)\n",
        "\n",
        "# 데이터셋 병합 실행\n",
        "merge_json_files(\n",
        "    json_dir=\"/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/train/json\",\n",
        "    image_dir=\"/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/train/images\",\n",
        "    output_json=\"/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/train/train_coco.json\"\n",
        ")\n",
        "merge_json_files(\n",
        "    json_dir=\"/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/val/json\",\n",
        "    image_dir=\"/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/val/images\",\n",
        "    output_json=\"/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/val/val_coco.json\"\n",
        ")\n",
        "merge_json_files(\n",
        "    json_dir=\"/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/test/json\",\n",
        "    image_dir=\"/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/test/images\",\n",
        "    output_json=\"/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/test/test_coco.json\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "jj4zQATIpA0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "# 데이터셋 등록\n",
        "register_coco_instances(\"train_dataset\", {}, \"/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/train/train_coco.json\", \"/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/train/images\")\n",
        "register_coco_instances(\"val_dataset\", {}, \"/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/val/val_coco.json\", \"/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/val/images\")\n",
        "register_coco_instances(\"test_dataset\", {}, \"/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/test/test_coco.json\", \"/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/test/images\")\n"
      ],
      "metadata": {
        "id": "2jqLiQ6KmFKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 이미지 폴더 경로\n",
        "image_dir = '/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/val/images'\n",
        "\n",
        "# 이미지 파일 리스트\n",
        "image_files = os.listdir(image_dir)\n",
        "\n",
        "# 파일 이름 수정 및 경로 수정\n",
        "for image_file in image_files:\n",
        "    if image_file.endswith(\"jpg.jpg\"):  # 현재 파일명 패턴 확인\n",
        "        # 불필요한 부분을 제거하여 새로운 파일명 생성\n",
        "        corrected_name = image_file.replace('jpg.jpg', '.jpg')\n",
        "\n",
        "        old_path = os.path.join(image_dir, image_file)\n",
        "        new_path = os.path.join(image_dir, corrected_name)\n",
        "\n",
        "        # 파일 이름 수정\n",
        "        os.rename(old_path, new_path)\n",
        "        print(f\"Renamed: {image_file} to {corrected_name}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcdMhE0-01fX",
        "outputId": "db3c88de-fbab-4d56-9d85-9963bbc3dabb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed: 04_03_20_6000236jpg.jpg to 04_03_20_6000236.jpg\n",
            "Renamed: 04_03_20_6000415jpg.jpg to 04_03_20_6000415.jpg\n",
            "Renamed: 04_03_20_6000124jpg.jpg to 04_03_20_6000124.jpg\n",
            "Renamed: 04_03_20_6000667jpg.jpg to 04_03_20_6000667.jpg\n",
            "Renamed: 04_03_20_6001176jpg.jpg to 04_03_20_6001176.jpg\n",
            "Renamed: 04_03_20_6000977jpg.jpg to 04_03_20_6000977.jpg\n",
            "Renamed: 04_03_20_6000160jpg.jpg to 04_03_20_6000160.jpg\n",
            "Renamed: 04_03_20_6000455jpg.jpg to 04_03_20_6000455.jpg\n",
            "Renamed: 04_03_20_6000123jpg.jpg to 04_03_20_6000123.jpg\n",
            "Renamed: 04_03_20_6000018jpg.jpg to 04_03_20_6000018.jpg\n",
            "Renamed: 04_03_20_6000666jpg.jpg to 04_03_20_6000666.jpg\n",
            "Renamed: 04_03_20_6000644jpg.jpg to 04_03_20_6000644.jpg\n",
            "Renamed: 04_03_20_6000992jpg.jpg to 04_03_20_6000992.jpg\n",
            "Renamed: 04_03_20_6001160jpg.jpg to 04_03_20_6001160.jpg\n",
            "Renamed: 04_03_20_6001037jpg.jpg to 04_03_20_6001037.jpg\n",
            "Renamed: 04_03_20_6000570jpg.jpg to 04_03_20_6000570.jpg\n",
            "Renamed: 04_03_20_6000757jpg.jpg to 04_03_20_6000757.jpg\n",
            "Renamed: 04_03_20_6001190jpg.jpg to 04_03_20_6001190.jpg\n",
            "Renamed: 04_03_20_6000238jpg.jpg to 04_03_20_6000238.jpg\n",
            "Renamed: 04_03_20_6000840jpg.jpg to 04_03_20_6000840.jpg\n",
            "Renamed: 04_03_20_6000445jpg.jpg to 04_03_20_6000445.jpg\n",
            "Renamed: 04_03_20_6000990jpg.jpg to 04_03_20_6000990.jpg\n",
            "Renamed: 04_03_20_6000673jpg.jpg to 04_03_20_6000673.jpg\n",
            "Renamed: 04_03_20_6000161jpg.jpg to 04_03_20_6000161.jpg\n",
            "Renamed: 04_03_20_6001150jpg.jpg to 04_03_20_6001150.jpg\n",
            "Renamed: 04_03_20_6000298jpg.jpg to 04_03_20_6000298.jpg\n",
            "Renamed: 04_03_20_6000081jpg.jpg to 04_03_20_6000081.jpg\n",
            "Renamed: 04_03_20_6000869jpg.jpg to 04_03_20_6000869.jpg\n",
            "Renamed: 04_03_20_6000864jpg.jpg to 04_03_20_6000864.jpg\n",
            "Renamed: 04_03_20_6000009jpg.jpg to 04_03_20_6000009.jpg\n",
            "Renamed: 04_03_20_6000333jpg.jpg to 04_03_20_6000333.jpg\n",
            "Renamed: 04_03_20_6000777jpg.jpg to 04_03_20_6000777.jpg\n",
            "Renamed: 04_03_20_6000054jpg.jpg to 04_03_20_6000054.jpg\n",
            "Renamed: 04_03_20_6000007jpg.jpg to 04_03_20_6000007.jpg\n",
            "Renamed: 04_03_20_6000097jpg.jpg to 04_03_20_6000097.jpg\n",
            "Renamed: 04_03_20_6000574jpg.jpg to 04_03_20_6000574.jpg\n",
            "Renamed: 04_03_20_6000102jpg.jpg to 04_03_20_6000102.jpg\n",
            "Renamed: 04_03_20_6000998jpg.jpg to 04_03_20_6000998.jpg\n",
            "Renamed: 04_03_20_6000206jpg.jpg to 04_03_20_6000206.jpg\n",
            "Renamed: 04_03_20_6001128jpg.jpg to 04_03_20_6001128.jpg\n",
            "Renamed: 04_03_20_6000674jpg.jpg to 04_03_20_6000674.jpg\n",
            "Renamed: 04_03_20_6000026jpg.jpg to 04_03_20_6000026.jpg\n",
            "Renamed: 04_03_20_6000107jpg.jpg to 04_03_20_6000107.jpg\n",
            "Renamed: 04_03_20_6000214jpg.jpg to 04_03_20_6000214.jpg\n",
            "Renamed: 04_03_20_6000274jpg.jpg to 04_03_20_6000274.jpg\n",
            "Renamed: 04_03_20_6000460jpg.jpg to 04_03_20_6000460.jpg\n",
            "Renamed: 04_03_20_6000738jpg.jpg to 04_03_20_6000738.jpg\n",
            "Renamed: 04_03_20_6000391jpg.jpg to 04_03_20_6000391.jpg\n",
            "Renamed: 04_03_20_6000041jpg.jpg to 04_03_20_6000041.jpg\n",
            "Renamed: 04_03_20_6000378jpg.jpg to 04_03_20_6000378.jpg\n",
            "Renamed: 04_03_20_6000227jpg.jpg to 04_03_20_6000227.jpg\n",
            "Renamed: 04_03_20_6001139jpg.jpg to 04_03_20_6001139.jpg\n",
            "Renamed: 04_03_20_6000920jpg.jpg to 04_03_20_6000920.jpg\n",
            "Renamed: 04_03_20_6000921jpg.jpg to 04_03_20_6000921.jpg\n",
            "Renamed: 04_03_20_6000854jpg.jpg to 04_03_20_6000854.jpg\n",
            "Renamed: 04_03_20_6000832jpg.jpg to 04_03_20_6000832.jpg\n",
            "Renamed: 04_03_20_6000802jpg.jpg to 04_03_20_6000802.jpg\n",
            "Renamed: 04_03_20_6000201jpg.jpg to 04_03_20_6000201.jpg\n",
            "Renamed: 04_03_20_6000324jpg.jpg to 04_03_20_6000324.jpg\n",
            "Renamed: 04_03_20_6000125jpg.jpg to 04_03_20_6000125.jpg\n",
            "Renamed: 04_03_20_6000958jpg.jpg to 04_03_20_6000958.jpg\n",
            "Renamed: 04_03_20_6000642jpg.jpg to 04_03_20_6000642.jpg\n",
            "Renamed: 04_03_20_6000963jpg.jpg to 04_03_20_6000963.jpg\n",
            "Renamed: 04_03_20_6000458jpg.jpg to 04_03_20_6000458.jpg\n",
            "Renamed: 04_03_20_6001127jpg.jpg to 04_03_20_6001127.jpg\n",
            "Renamed: 04_03_20_6000708jpg.jpg to 04_03_20_6000708.jpg\n",
            "Renamed: 04_03_20_6000395jpg.jpg to 04_03_20_6000395.jpg\n",
            "Renamed: 04_03_20_6000188jpg.jpg to 04_03_20_6000188.jpg\n",
            "Renamed: 04_03_20_6000545jpg.jpg to 04_03_20_6000545.jpg\n",
            "Renamed: 04_03_20_6000275jpg.jpg to 04_03_20_6000275.jpg\n",
            "Renamed: 04_03_20_6000136jpg.jpg to 04_03_20_6000136.jpg\n",
            "Renamed: 04_03_20_6000951jpg.jpg to 04_03_20_6000951.jpg\n",
            "Renamed: 04_03_20_6001011jpg.jpg to 04_03_20_6001011.jpg\n",
            "Renamed: 04_03_20_6000006jpg.jpg to 04_03_20_6000006.jpg\n",
            "Renamed: 04_03_20_6000066jpg.jpg to 04_03_20_6000066.jpg\n",
            "Renamed: 04_03_20_6000727jpg.jpg to 04_03_20_6000727.jpg\n",
            "Renamed: 04_03_20_6000098jpg.jpg to 04_03_20_6000098.jpg\n",
            "Renamed: 04_03_20_6000640jpg.jpg to 04_03_20_6000640.jpg\n",
            "Renamed: 04_03_20_6000249jpg.jpg to 04_03_20_6000249.jpg\n",
            "Renamed: 04_03_20_6000143jpg.jpg to 04_03_20_6000143.jpg\n",
            "Renamed: 04_03_20_6000901jpg.jpg to 04_03_20_6000901.jpg\n",
            "Renamed: 04_03_20_6000946jpg.jpg to 04_03_20_6000946.jpg\n",
            "Renamed: 04_03_20_6000535jpg.jpg to 04_03_20_6000535.jpg\n",
            "Renamed: 04_03_20_6000925jpg.jpg to 04_03_20_6000925.jpg\n",
            "Renamed: 04_03_20_6000356jpg.jpg to 04_03_20_6000356.jpg\n",
            "Renamed: 04_03_20_6000762jpg.jpg to 04_03_20_6000762.jpg\n",
            "Renamed: 04_03_20_6000228jpg.jpg to 04_03_20_6000228.jpg\n",
            "Renamed: 04_03_20_6000522jpg.jpg to 04_03_20_6000522.jpg\n",
            "Renamed: 04_03_20_6000439jpg.jpg to 04_03_20_6000439.jpg\n",
            "Renamed: 04_03_20_6000404jpg.jpg to 04_03_20_6000404.jpg\n",
            "Renamed: 04_03_20_6000199jpg.jpg to 04_03_20_6000199.jpg\n",
            "Renamed: 04_03_20_6000464jpg.jpg to 04_03_20_6000464.jpg\n",
            "Renamed: 04_03_20_6001126jpg.jpg to 04_03_20_6001126.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultTrainer\n",
        "import json\n",
        "\n",
        "def get_num_classes(coco_json_path):\n",
        "    \"\"\"\n",
        "    COCO JSON 파일에서 카테고리 개수를 반환.\n",
        "    Args:\n",
        "        coco_json_path (str): COCO JSON 파일 경로\n",
        "    Returns:\n",
        "        int: 카테고리 개수\n",
        "    \"\"\"\n",
        "    with open(coco_json_path, 'r') as f:\n",
        "        coco_data = json.load(f)\n",
        "    return len(coco_data[\"categories\"])\n",
        "\n",
        "# train_coco.json에서 카테고리 개수 읽기\n",
        "num_classes = get_num_classes(\"/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/train/train_coco.json\")\n",
        "\n",
        "# Config 설정\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\"/content/drive/MyDrive/Dataset/자연재해로 인한 생활시설 안전 데이터/112.자연재해로 인한 생활시설 안전 데이터(AI모델)/① 모델소스코드/detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.DATASETS.TRAIN=(\"train_dataset\", )\n",
        "cfg.DATASETS.TEST=(\"test_dataset\", )\n",
        "cfg.DATALOADER.NUM_WORKERS = 4\n",
        "cfg.MODEL.WEIGHTS=\"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 1000\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes\n",
        "\n",
        "# 모델 훈련\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7VhDt94rSxB",
        "outputId": "56b0fe0b-16a0-42f2-e722-9c443b9b94a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11/28 03:41:39 d2.engine.defaults]: Model:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "WARNING [11/28 03:41:39 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[11/28 03:41:39 d2.data.datasets.coco]: Loaded 744 images in COCO format from /content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/train/train_coco.json\n",
            "[11/28 03:41:39 d2.data.build]: Removed 0 images with no usable annotations. 744 images left.\n",
            "[11/28 03:41:39 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[11/28 03:41:39 d2.data.build]: Using training sampler TrainingSampler\n",
            "[11/28 03:41:39 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[11/28 03:41:39 d2.data.common]: Serializing 744 elements to byte tensors and concatenating them all ...\n",
            "[11/28 03:41:39 d2.data.common]: Serialized dataset takes 3.25 MiB\n",
            "[11/28 03:41:39 d2.data.build]: Making batched data loader with batch_size=2\n",
            "WARNING [11/28 03:41:39 d2.solver.build]: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
            "[11/28 03:41:39 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (12, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (12,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (3, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (3,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
            "roi_heads.box_predictor.cls_score.{bias, weight}\n",
            "roi_heads.mask_head.predictor.{bias, weight}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11/28 03:41:39 d2.engine.train_loop]: Starting training from iteration 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11/28 03:41:56 d2.utils.events]:  eta: 0:09:22  iter: 19  total_loss: 2.648  loss_cls: 1.326  loss_box_reg: 0.5598  loss_mask: 0.6995  loss_rpn_cls: 0.02128  loss_rpn_loc: 0.03294    time: 0.5800  last_time: 0.5290  data_time: 0.0708  last_data_time: 0.0111   lr: 4.9953e-06  max_mem: 2753M\n",
            "[11/28 03:42:11 d2.utils.events]:  eta: 0:08:56  iter: 39  total_loss: 2.506  loss_cls: 1.253  loss_box_reg: 0.4735  loss_mask: 0.6926  loss_rpn_cls: 0.01399  loss_rpn_loc: 0.03313    time: 0.5730  last_time: 0.4538  data_time: 0.0155  last_data_time: 0.0063   lr: 9.9902e-06  max_mem: 2753M\n",
            "[11/28 03:42:22 d2.utils.events]:  eta: 0:08:44  iter: 59  total_loss: 2.351  loss_cls: 1.127  loss_box_reg: 0.4577  loss_mask: 0.6848  loss_rpn_cls: 0.01752  loss_rpn_loc: 0.01953    time: 0.5675  last_time: 0.6198  data_time: 0.0082  last_data_time: 0.0171   lr: 1.4985e-05  max_mem: 2753M\n",
            "[11/28 03:42:33 d2.utils.events]:  eta: 0:08:30  iter: 79  total_loss: 2.113  loss_cls: 0.9229  loss_box_reg: 0.5356  loss_mask: 0.6671  loss_rpn_cls: 0.01256  loss_rpn_loc: 0.0185    time: 0.5604  last_time: 0.5020  data_time: 0.0068  last_data_time: 0.0056   lr: 1.998e-05  max_mem: 2753M\n",
            "[11/28 03:42:45 d2.utils.events]:  eta: 0:08:23  iter: 99  total_loss: 1.912  loss_cls: 0.7766  loss_box_reg: 0.4226  loss_mask: 0.651  loss_rpn_cls: 0.02491  loss_rpn_loc: 0.02734    time: 0.5637  last_time: 0.5789  data_time: 0.0141  last_data_time: 0.0071   lr: 2.4975e-05  max_mem: 2753M\n",
            "[11/28 03:42:56 d2.utils.events]:  eta: 0:08:14  iter: 119  total_loss: 1.896  loss_cls: 0.6404  loss_box_reg: 0.5328  loss_mask: 0.6325  loss_rpn_cls: 0.0171  loss_rpn_loc: 0.02678    time: 0.5648  last_time: 0.5777  data_time: 0.0146  last_data_time: 0.0349   lr: 2.997e-05  max_mem: 2753M\n",
            "[11/28 03:43:07 d2.utils.events]:  eta: 0:07:58  iter: 139  total_loss: 1.755  loss_cls: 0.5525  loss_box_reg: 0.507  loss_mask: 0.5947  loss_rpn_cls: 0.01332  loss_rpn_loc: 0.02765    time: 0.5619  last_time: 0.5319  data_time: 0.0077  last_data_time: 0.0072   lr: 3.4965e-05  max_mem: 2754M\n",
            "[11/28 03:43:18 d2.utils.events]:  eta: 0:07:47  iter: 159  total_loss: 1.662  loss_cls: 0.4973  loss_box_reg: 0.4835  loss_mask: 0.5679  loss_rpn_cls: 0.01455  loss_rpn_loc: 0.02012    time: 0.5623  last_time: 0.5486  data_time: 0.0084  last_data_time: 0.0057   lr: 3.996e-05  max_mem: 2754M\n",
            "[11/28 03:43:29 d2.utils.events]:  eta: 0:07:36  iter: 179  total_loss: 1.479  loss_cls: 0.4487  loss_box_reg: 0.4603  loss_mask: 0.5361  loss_rpn_cls: 0.0142  loss_rpn_loc: 0.01968    time: 0.5626  last_time: 0.4943  data_time: 0.0087  last_data_time: 0.0062   lr: 4.4955e-05  max_mem: 2754M\n",
            "[11/28 03:43:41 d2.utils.events]:  eta: 0:07:25  iter: 199  total_loss: 1.404  loss_cls: 0.3911  loss_box_reg: 0.496  loss_mask: 0.4994  loss_rpn_cls: 0.01134  loss_rpn_loc: 0.03005    time: 0.5619  last_time: 0.6171  data_time: 0.0079  last_data_time: 0.0091   lr: 4.995e-05  max_mem: 2754M\n",
            "[11/28 03:43:52 d2.utils.events]:  eta: 0:07:14  iter: 219  total_loss: 1.398  loss_cls: 0.383  loss_box_reg: 0.4891  loss_mask: 0.4965  loss_rpn_cls: 0.01192  loss_rpn_loc: 0.01994    time: 0.5620  last_time: 0.5581  data_time: 0.0098  last_data_time: 0.0073   lr: 5.4945e-05  max_mem: 2754M\n",
            "[11/28 03:44:03 d2.utils.events]:  eta: 0:07:03  iter: 239  total_loss: 1.195  loss_cls: 0.3162  loss_box_reg: 0.3922  loss_mask: 0.3979  loss_rpn_cls: 0.01363  loss_rpn_loc: 0.01484    time: 0.5632  last_time: 0.5401  data_time: 0.0099  last_data_time: 0.0068   lr: 5.994e-05  max_mem: 2754M\n",
            "[11/28 03:44:16 d2.utils.events]:  eta: 0:06:54  iter: 259  total_loss: 1.293  loss_cls: 0.3419  loss_box_reg: 0.5141  loss_mask: 0.3797  loss_rpn_cls: 0.01344  loss_rpn_loc: 0.02647    time: 0.5665  last_time: 0.6141  data_time: 0.0204  last_data_time: 0.0169   lr: 6.4935e-05  max_mem: 2754M\n",
            "[11/28 03:44:26 d2.utils.events]:  eta: 0:06:40  iter: 279  total_loss: 1.134  loss_cls: 0.3004  loss_box_reg: 0.436  loss_mask: 0.3654  loss_rpn_cls: 0.01478  loss_rpn_loc: 0.02779    time: 0.5643  last_time: 0.5544  data_time: 0.0082  last_data_time: 0.0061   lr: 6.993e-05  max_mem: 2754M\n",
            "[11/28 03:44:38 d2.utils.events]:  eta: 0:06:29  iter: 299  total_loss: 1.054  loss_cls: 0.2667  loss_box_reg: 0.4151  loss_mask: 0.2975  loss_rpn_cls: 0.005672  loss_rpn_loc: 0.02403    time: 0.5650  last_time: 0.5470  data_time: 0.0129  last_data_time: 0.0085   lr: 7.4925e-05  max_mem: 2754M\n",
            "[11/28 03:44:49 d2.utils.events]:  eta: 0:06:19  iter: 319  total_loss: 1.132  loss_cls: 0.3009  loss_box_reg: 0.4885  loss_mask: 0.2751  loss_rpn_cls: 0.004586  loss_rpn_loc: 0.01724    time: 0.5649  last_time: 0.5683  data_time: 0.0107  last_data_time: 0.0056   lr: 7.992e-05  max_mem: 2754M\n",
            "[11/28 03:45:01 d2.utils.events]:  eta: 0:06:08  iter: 339  total_loss: 1.292  loss_cls: 0.3081  loss_box_reg: 0.5469  loss_mask: 0.2739  loss_rpn_cls: 0.01402  loss_rpn_loc: 0.03386    time: 0.5652  last_time: 0.6393  data_time: 0.0095  last_data_time: 0.0399   lr: 8.4915e-05  max_mem: 2754M\n",
            "[11/28 03:45:12 d2.utils.events]:  eta: 0:05:57  iter: 359  total_loss: 1.026  loss_cls: 0.269  loss_box_reg: 0.5401  loss_mask: 0.2256  loss_rpn_cls: 0.002585  loss_rpn_loc: 0.02102    time: 0.5643  last_time: 0.5679  data_time: 0.0085  last_data_time: 0.0095   lr: 8.991e-05  max_mem: 2754M\n",
            "[11/28 03:45:23 d2.utils.events]:  eta: 0:05:45  iter: 379  total_loss: 0.9647  loss_cls: 0.2202  loss_box_reg: 0.4832  loss_mask: 0.2506  loss_rpn_cls: 0.009988  loss_rpn_loc: 0.02637    time: 0.5643  last_time: 0.5474  data_time: 0.0120  last_data_time: 0.0119   lr: 9.4905e-05  max_mem: 2754M\n",
            "[11/28 03:45:34 d2.utils.events]:  eta: 0:05:34  iter: 399  total_loss: 0.9756  loss_cls: 0.2241  loss_box_reg: 0.4224  loss_mask: 0.2707  loss_rpn_cls: 0.01049  loss_rpn_loc: 0.0308    time: 0.5639  last_time: 0.6482  data_time: 0.0088  last_data_time: 0.0067   lr: 9.99e-05  max_mem: 2754M\n",
            "[11/28 03:45:44 d2.utils.events]:  eta: 0:05:22  iter: 419  total_loss: 1.103  loss_cls: 0.3047  loss_box_reg: 0.4969  loss_mask: 0.2656  loss_rpn_cls: 0.01337  loss_rpn_loc: 0.0312    time: 0.5619  last_time: 0.5043  data_time: 0.0068  last_data_time: 0.0065   lr: 0.0001049  max_mem: 2754M\n",
            "[11/28 03:45:56 d2.utils.events]:  eta: 0:05:11  iter: 439  total_loss: 0.9361  loss_cls: 0.239  loss_box_reg: 0.448  loss_mask: 0.2283  loss_rpn_cls: 0.005917  loss_rpn_loc: 0.02833    time: 0.5620  last_time: 0.5650  data_time: 0.0109  last_data_time: 0.0059   lr: 0.00010989  max_mem: 2754M\n",
            "[11/28 03:46:07 d2.utils.events]:  eta: 0:05:00  iter: 459  total_loss: 0.9157  loss_cls: 0.2008  loss_box_reg: 0.3429  loss_mask: 0.2236  loss_rpn_cls: 0.003886  loss_rpn_loc: 0.02819    time: 0.5616  last_time: 0.4793  data_time: 0.0114  last_data_time: 0.0065   lr: 0.00011489  max_mem: 2754M\n",
            "[11/28 03:46:17 d2.utils.events]:  eta: 0:04:48  iter: 479  total_loss: 0.9493  loss_cls: 0.2401  loss_box_reg: 0.434  loss_mask: 0.217  loss_rpn_cls: 0.005195  loss_rpn_loc: 0.01423    time: 0.5595  last_time: 0.5416  data_time: 0.0068  last_data_time: 0.0059   lr: 0.00011988  max_mem: 2754M\n",
            "[11/28 03:46:28 d2.utils.events]:  eta: 0:04:37  iter: 499  total_loss: 0.9027  loss_cls: 0.2147  loss_box_reg: 0.3701  loss_mask: 0.2062  loss_rpn_cls: 0.002039  loss_rpn_loc: 0.01534    time: 0.5595  last_time: 0.5177  data_time: 0.0115  last_data_time: 0.0065   lr: 0.00012488  max_mem: 2754M\n",
            "[11/28 03:46:39 d2.utils.events]:  eta: 0:04:25  iter: 519  total_loss: 0.7859  loss_cls: 0.1828  loss_box_reg: 0.2986  loss_mask: 0.2168  loss_rpn_cls: 0.00443  loss_rpn_loc: 0.01389    time: 0.5596  last_time: 0.5230  data_time: 0.0114  last_data_time: 0.0055   lr: 0.00012987  max_mem: 2754M\n",
            "[11/28 03:46:51 d2.utils.events]:  eta: 0:04:14  iter: 539  total_loss: 0.8272  loss_cls: 0.2132  loss_box_reg: 0.3517  loss_mask: 0.1916  loss_rpn_cls: 0.007324  loss_rpn_loc: 0.01149    time: 0.5597  last_time: 0.5944  data_time: 0.0073  last_data_time: 0.0084   lr: 0.00013487  max_mem: 2754M\n",
            "[11/28 03:47:02 d2.utils.events]:  eta: 0:04:03  iter: 559  total_loss: 0.8501  loss_cls: 0.2649  loss_box_reg: 0.3618  loss_mask: 0.2083  loss_rpn_cls: 0.01017  loss_rpn_loc: 0.02036    time: 0.5593  last_time: 0.5384  data_time: 0.0084  last_data_time: 0.0062   lr: 0.00013986  max_mem: 2754M\n",
            "[11/28 03:47:13 d2.utils.events]:  eta: 0:03:52  iter: 579  total_loss: 0.7224  loss_cls: 0.2322  loss_box_reg: 0.3132  loss_mask: 0.1844  loss_rpn_cls: 0.009786  loss_rpn_loc: 0.01721    time: 0.5597  last_time: 0.5181  data_time: 0.0122  last_data_time: 0.0099   lr: 0.00014486  max_mem: 2754M\n",
            "[11/28 03:47:25 d2.utils.events]:  eta: 0:03:41  iter: 599  total_loss: 0.7751  loss_cls: 0.2098  loss_box_reg: 0.2726  loss_mask: 0.1948  loss_rpn_cls: 0.005348  loss_rpn_loc: 0.01893    time: 0.5601  last_time: 0.5566  data_time: 0.0125  last_data_time: 0.0072   lr: 0.00014985  max_mem: 2754M\n",
            "[11/28 03:47:35 d2.utils.events]:  eta: 0:03:30  iter: 619  total_loss: 0.6497  loss_cls: 0.1781  loss_box_reg: 0.2118  loss_mask: 0.2293  loss_rpn_cls: 0.003681  loss_rpn_loc: 0.02607    time: 0.5590  last_time: 0.5496  data_time: 0.0072  last_data_time: 0.0066   lr: 0.00015485  max_mem: 2754M\n",
            "[11/28 03:47:47 d2.utils.events]:  eta: 0:03:19  iter: 639  total_loss: 0.7303  loss_cls: 0.2506  loss_box_reg: 0.2958  loss_mask: 0.1815  loss_rpn_cls: 0.01234  loss_rpn_loc: 0.01332    time: 0.5593  last_time: 0.5067  data_time: 0.0109  last_data_time: 0.0075   lr: 0.00015984  max_mem: 2754M\n",
            "[11/28 03:47:58 d2.utils.events]:  eta: 0:03:08  iter: 659  total_loss: 0.7546  loss_cls: 0.233  loss_box_reg: 0.2639  loss_mask: 0.2158  loss_rpn_cls: 0.006004  loss_rpn_loc: 0.02279    time: 0.5594  last_time: 0.5517  data_time: 0.0142  last_data_time: 0.0070   lr: 0.00016484  max_mem: 2756M\n",
            "[11/28 03:48:09 d2.utils.events]:  eta: 0:02:56  iter: 679  total_loss: 0.6949  loss_cls: 0.237  loss_box_reg: 0.2533  loss_mask: 0.1742  loss_rpn_cls: 0.008071  loss_rpn_loc: 0.02216    time: 0.5587  last_time: 0.6366  data_time: 0.0071  last_data_time: 0.0058   lr: 0.00016983  max_mem: 2756M\n",
            "[11/28 03:48:20 d2.utils.events]:  eta: 0:02:46  iter: 699  total_loss: 0.6838  loss_cls: 0.2411  loss_box_reg: 0.2811  loss_mask: 0.1752  loss_rpn_cls: 0.005575  loss_rpn_loc: 0.02409    time: 0.5595  last_time: 0.5258  data_time: 0.0154  last_data_time: 0.0057   lr: 0.00017483  max_mem: 2756M\n",
            "[11/28 03:48:32 d2.utils.events]:  eta: 0:02:34  iter: 719  total_loss: 0.6069  loss_cls: 0.1856  loss_box_reg: 0.1956  loss_mask: 0.1465  loss_rpn_cls: 0.006833  loss_rpn_loc: 0.04125    time: 0.5598  last_time: 0.5200  data_time: 0.0117  last_data_time: 0.0060   lr: 0.00017982  max_mem: 2756M\n",
            "[11/28 03:48:43 d2.utils.events]:  eta: 0:02:23  iter: 739  total_loss: 0.6862  loss_cls: 0.2086  loss_box_reg: 0.2352  loss_mask: 0.201  loss_rpn_cls: 0.006793  loss_rpn_loc: 0.02167    time: 0.5600  last_time: 0.6187  data_time: 0.0096  last_data_time: 0.0084   lr: 0.00018482  max_mem: 2756M\n",
            "[11/28 03:48:53 d2.utils.events]:  eta: 0:02:12  iter: 759  total_loss: 0.575  loss_cls: 0.1899  loss_box_reg: 0.1759  loss_mask: 0.1726  loss_rpn_cls: 0.006171  loss_rpn_loc: 0.02735    time: 0.5589  last_time: 0.5798  data_time: 0.0064  last_data_time: 0.0078   lr: 0.00018981  max_mem: 2756M\n",
            "[11/28 03:49:05 d2.utils.events]:  eta: 0:02:01  iter: 779  total_loss: 0.6731  loss_cls: 0.2358  loss_box_reg: 0.1844  loss_mask: 0.1984  loss_rpn_cls: 0.005153  loss_rpn_loc: 0.0243    time: 0.5594  last_time: 0.5425  data_time: 0.0130  last_data_time: 0.0066   lr: 0.00019481  max_mem: 2756M\n",
            "[11/28 03:49:16 d2.utils.events]:  eta: 0:01:50  iter: 799  total_loss: 0.7196  loss_cls: 0.2314  loss_box_reg: 0.2584  loss_mask: 0.216  loss_rpn_cls: 0.008524  loss_rpn_loc: 0.02596    time: 0.5595  last_time: 0.5289  data_time: 0.0119  last_data_time: 0.0066   lr: 0.0001998  max_mem: 2756M\n",
            "[11/28 03:49:27 d2.utils.events]:  eta: 0:01:39  iter: 819  total_loss: 0.5965  loss_cls: 0.1818  loss_box_reg: 0.2114  loss_mask: 0.1702  loss_rpn_cls: 0.001918  loss_rpn_loc: 0.02658    time: 0.5591  last_time: 0.5794  data_time: 0.0076  last_data_time: 0.0086   lr: 0.0002048  max_mem: 2756M\n",
            "[11/28 03:49:38 d2.utils.events]:  eta: 0:01:28  iter: 839  total_loss: 0.5831  loss_cls: 0.1647  loss_box_reg: 0.1638  loss_mask: 0.1706  loss_rpn_cls: 0.003379  loss_rpn_loc: 0.01498    time: 0.5586  last_time: 0.5243  data_time: 0.0085  last_data_time: 0.0070   lr: 0.00020979  max_mem: 2756M\n",
            "[11/28 03:49:50 d2.utils.events]:  eta: 0:01:17  iter: 859  total_loss: 0.7462  loss_cls: 0.2443  loss_box_reg: 0.2766  loss_mask: 0.1943  loss_rpn_cls: 0.006081  loss_rpn_loc: 0.02645    time: 0.5592  last_time: 0.5661  data_time: 0.0106  last_data_time: 0.0307   lr: 0.00021479  max_mem: 2756M\n",
            "[11/28 03:50:01 d2.utils.events]:  eta: 0:01:06  iter: 879  total_loss: 0.6179  loss_cls: 0.257  loss_box_reg: 0.177  loss_mask: 0.1449  loss_rpn_cls: 0.003794  loss_rpn_loc: 0.01505    time: 0.5594  last_time: 0.5912  data_time: 0.0104  last_data_time: 0.0220   lr: 0.00021978  max_mem: 2756M\n",
            "[11/28 03:50:12 d2.utils.events]:  eta: 0:00:55  iter: 899  total_loss: 0.6863  loss_cls: 0.2471  loss_box_reg: 0.2368  loss_mask: 0.1633  loss_rpn_cls: 0.003127  loss_rpn_loc: 0.0275    time: 0.5589  last_time: 0.6078  data_time: 0.0073  last_data_time: 0.0069   lr: 0.00022478  max_mem: 2756M\n",
            "[11/28 03:50:23 d2.utils.events]:  eta: 0:00:44  iter: 919  total_loss: 0.6621  loss_cls: 0.2263  loss_box_reg: 0.2545  loss_mask: 0.1825  loss_rpn_cls: 0.00592  loss_rpn_loc: 0.02218    time: 0.5590  last_time: 0.5466  data_time: 0.0085  last_data_time: 0.0064   lr: 0.00022977  max_mem: 2756M\n",
            "[11/28 03:50:34 d2.utils.events]:  eta: 0:00:33  iter: 939  total_loss: 0.6154  loss_cls: 0.2076  loss_box_reg: 0.2358  loss_mask: 0.2006  loss_rpn_cls: 0.006537  loss_rpn_loc: 0.01634    time: 0.5592  last_time: 0.5504  data_time: 0.0131  last_data_time: 0.0068   lr: 0.00023477  max_mem: 2756M\n",
            "[11/28 03:50:45 d2.utils.events]:  eta: 0:00:22  iter: 959  total_loss: 0.6036  loss_cls: 0.2194  loss_box_reg: 0.2085  loss_mask: 0.1789  loss_rpn_cls: 0.002126  loss_rpn_loc: 0.01859    time: 0.5589  last_time: 0.5777  data_time: 0.0092  last_data_time: 0.0360   lr: 0.00023976  max_mem: 2756M\n",
            "[11/28 03:50:56 d2.utils.events]:  eta: 0:00:11  iter: 979  total_loss: 0.6569  loss_cls: 0.252  loss_box_reg: 0.2083  loss_mask: 0.1825  loss_rpn_cls: 0.001749  loss_rpn_loc: 0.01716    time: 0.5588  last_time: 0.5559  data_time: 0.0089  last_data_time: 0.0074   lr: 0.00024476  max_mem: 2756M\n",
            "[11/28 03:51:09 d2.utils.events]:  eta: 0:00:00  iter: 999  total_loss: 0.6927  loss_cls: 0.2116  loss_box_reg: 0.237  loss_mask: 0.1629  loss_rpn_cls: 0.003375  loss_rpn_loc: 0.02129    time: 0.5590  last_time: 0.5513  data_time: 0.0107  last_data_time: 0.0069   lr: 0.00024975  max_mem: 2756M\n",
            "[11/28 03:51:09 d2.engine.hooks]: Overall training speed: 998 iterations in 0:09:17 (0.5590 s / it)\n",
            "[11/28 03:51:09 d2.engine.hooks]: Total training time: 0:09:23 (0:00:05 on hooks)\n",
            "WARNING [11/28 03:51:09 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[11/28 03:51:09 d2.data.datasets.coco]: Loaded 93 images in COCO format from /content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/test/test_coco.json\n",
            "[11/28 03:51:09 d2.data.build]: Distribution of instances among all 2 categories:\n",
            "|   category    | #instances   |   category    | #instances   |\n",
            "|:-------------:|:-------------|:-------------:|:-------------|\n",
            "| heavy rain_.. | 146          | heavy rain_.. | 146          |\n",
            "|               |              |               |              |\n",
            "|     total     | 292          |               |              |\n",
            "[11/28 03:51:09 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[11/28 03:51:09 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[11/28 03:51:09 d2.data.common]: Serializing 93 elements to byte tensors and concatenating them all ...\n",
            "[11/28 03:51:09 d2.data.common]: Serialized dataset takes 0.38 MiB\n",
            "WARNING [11/28 03:51:09 d2.engine.defaults]: No evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "# 저장된 모델 경로\n",
        "model_weights_path = \"/content/output/model_final.pth\"\n",
        "\n",
        "# COCO JSON 파일에서 카테고리 개수 읽기\n",
        "def get_num_classes(coco_json_path):\n",
        "    with open(coco_json_path, 'r') as f:\n",
        "        coco_data = json.load(f)\n",
        "    return len(coco_data[\"categories\"])\n",
        "\n",
        "num_classes = get_num_classes(\"/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/train/train_coco.json\")\n",
        "\n",
        "# Config 설정\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\"/content/drive/MyDrive/Dataset/자연재해로 인한 생활시설 안전 데이터/112.자연재해로 인한 생활시설 안전 데이터(AI모델)/① 모델소스코드/detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.MODEL.WEIGHTS = model_weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes  # num_classes 설정\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # 추론 시 threshold 설정\n",
        "cfg.DATASETS.TEST = (\"test_dataset\",)\n",
        "\n",
        "# Predictor 생성\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# 예시 이미지 경로\n",
        "image_path = '/content/drive/MyDrive/Dataset/dataset/GDSC_Sprint_Challenge/dataset/test/images/04_03_20_6000049.jpg'\n",
        "\n",
        "# 이미지 읽기\n",
        "im = cv2.imread(image_path)\n",
        "\n",
        "# 추론\n",
        "outputs = predictor(im)\n",
        "\n",
        "# 심각도 측정 기준 (확신도에 따라)\n",
        "def measure_severity(scores):\n",
        "    severity = []\n",
        "    for score in scores:\n",
        "        if score > 0.9:\n",
        "            severity.append(\"High Severity\")\n",
        "        elif score > 0.5:\n",
        "            severity.append(\"Medium Severity\")\n",
        "        else:\n",
        "            severity.append(\"Low Severity\")\n",
        "    return severity\n",
        "\n",
        "# 예측 결과의 확신도 (scores) 가져오기\n",
        "scores = outputs[\"instances\"].scores\n",
        "\n",
        "# 심각도 측정\n",
        "severity = measure_severity(scores)\n",
        "\n",
        "# 결과 출력\n",
        "for idx, score in enumerate(scores):\n",
        "    print(f\"Object {idx + 1}: {severity[idx]} (Confidence Score: {score:.2f})\")\n",
        "\n",
        "# 결과 시각화\n",
        "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(\"test_dataset\"), scale=1.2)\n",
        "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "result_image = v.get_image()[:, :, ::-1]\n",
        "\n",
        "# 결과 이미지 저장\n",
        "cv2.imwrite(\"/content/output/result_image_04_03_20_6000049.jpg\", result_image)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqei3KJcspZb",
        "outputId": "41f9d5eb-d833-4b64-fa65-e181a9b078b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11/28 03:59:05 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from /content/output/model_final.pth ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/fvcore/common/checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object 1: Medium Severity (Confidence Score: 0.82)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6pAH0TDR64UB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}